{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Tg8YBAMNePid"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlfFssIMd4z7"
      },
      "source": [
        "<center><h1>HW2: Collaborative Filtering</h1></center>\n",
        "<hr>\n",
        "\n",
        "Name: **Firas Jolha**\n",
        "\n",
        "Email: **f.jolha@innopolis.university**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg8YBAMNePid"
      },
      "source": [
        "# Install & Import Libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtd4ldoCeO6x"
      },
      "source": [
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from torch import nn\n",
        "import torch.nn.functional as f\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from os.path import join as path_join\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyX9FjOveUmO"
      },
      "source": [
        "# Data Preparation & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "brotvfESoxOd",
        "outputId": "3af8aa72-00e8-44f3-aac4-3785935effde"
      },
      "source": [
        "# Set Data Path\n",
        "DATA_PATH = \"data\"\n",
        "DATA_PATH = \".\"\n",
        " \n",
        "# Read Training and Test Data\n",
        "train_df = pd.read_csv(path_join(DATA_PATH, \"train.csv\"))\n",
        "test_df = pd.read_csv(path_join(DATA_PATH, \"test.csv\"))\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>47</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>253</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>260</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   userId  movieId  rating\n",
              "0       1       32     3.5\n",
              "1       1       47     3.5\n",
              "2       1       50     3.5\n",
              "3       1      253     4.0\n",
              "4       1      260     4.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyOPgdCbpHAO",
        "outputId": "9179ded1-c1fb-4df5-c96f-62a3c4399cfd"
      },
      "source": [
        "train_df.shape, test_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((761972, 3), (190819, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYaNnLEBH_8d"
      },
      "source": [
        "# Data Exploration and Preprocessing\n",
        " \n",
        "user_ids = train_df['userId']\n",
        "movie_ids = train_df['movieId']\n",
        "ratings = train_df['rating']\n",
        " \n",
        "def map_ids(series):\n",
        "  '''Resets ids of both users and items.\n",
        " \n",
        "  Args:\n",
        "    series (pd.Series): The series of ids to be converted\n",
        " \n",
        "  Returns:\n",
        "    pd.Series: a series of the same type and attributes after resetting the ids.\n",
        " \n",
        "  '''\n",
        " \n",
        "  uq = series.unique()\n",
        " \n",
        "  return series.map(pd.Series(range(uq.size), index = uq))\n",
        " \n",
        "def unmap_ids(series, original_series):  \n",
        "  '''Returns back the original ids of series being converted by map_ids function.\n",
        " \n",
        "  Args:\n",
        "    series (pd.Series): The series of ids to be returned back.\n",
        "    original_series (pd.Series): The series of ids from the original data frame.\n",
        " \n",
        "  Returns:\n",
        "    pd.Series: a series of the same type and attributes after returning the ids to the original values.\n",
        " \n",
        "  '''\n",
        "  \n",
        "  uq = original_series.unique()\n",
        " \n",
        "  return series.map(pd.Series(uq, index = range(uq.size)))\n",
        " \n",
        "# Resetting the ids of training data\n",
        "user_ids = map_ids(user_ids)\n",
        "movie_ids = map_ids(movie_ids)\n",
        " \n",
        "# Resetting the ids of test data\n",
        "test_user_ids = map_ids(test_df['userId']) \n",
        "test_movie_ids = map_ids(test_df['movieId']) \n",
        "test_ratings = test_df['rating']\n",
        " \n",
        "# Statistics of training data\n",
        "n_users = np.max(user_ids) + 1\n",
        "n_movies = np.max(movie_ids) + 1\n",
        " \n",
        "# Statistics of test data\n",
        "test_n_users = np.max(test_user_ids) + 1\n",
        "test_n_movies = np.max(test_movie_ids) + 1\n",
        " \n",
        "# Returning the indices back can be done using unmap_ids function\n",
        "# unmap_ids(movie_ids, train_df['movieId'])\n",
        "# unmap_ids(user_ids, train_df['userId'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMve3Plg9p7y"
      },
      "source": [
        "# Basic Collaborative Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sNY1y6DbNTo"
      },
      "source": [
        "## Sparse Rating Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwJub_U9bNHQ",
        "outputId": "aaa1dffe-3d82-45b8-a6c7-9823a58e7ade"
      },
      "source": [
        "# Define the training rating matrix as sparse matrix\n",
        " \n",
        "R = sparse.coo_matrix(\n",
        "    (ratings, (user_ids, movie_ids)),\n",
        "    shape=(n_users, n_movies), \n",
        "    dtype=np.float\n",
        " )\n",
        " \n",
        "R"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<6687x5064 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 761972 stored elements in COOrdinate format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7oSqKgX4zy8",
        "outputId": "30fc52f4-369b-42ac-df5c-db988c848cd7"
      },
      "source": [
        "# Define the rating matrix for test data as sparse matrix\n",
        " \n",
        "R2 = sparse.coo_matrix(\n",
        "    (test_ratings, (test_user_ids, test_movie_ids)),\n",
        "    shape=(test_n_users, test_n_movies), \n",
        "    dtype=np.float\n",
        ")\n",
        " \n",
        "R2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<6674x5059 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 190819 stored elements in COOrdinate format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljTzElJA9z40"
      },
      "source": [
        "## Matrix Factorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJb129lR9--7"
      },
      "source": [
        "## ALS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJRtya0TvtLA"
      },
      "source": [
        " \n",
        "def update_P(P, Q, R, alpha = 0.001, lam = 0.001):\n",
        "  '''Updates the values of matrix P\n",
        " \n",
        "  Args:\n",
        "    P: \n",
        " \n",
        "  Returns:\n",
        "    np.ndarray: P itself after adjusting the values\n",
        " \n",
        "  '''\n",
        " \n",
        "  # assert R.shape == R_hat.shape, \"R shoud equal to R_hat\"\n",
        "  assert P.shape[1] == Q.shape[1], \"P and Q should have proper dimensions for matrix multiplication\"\n",
        " \n",
        "  M = np.zeros(R.shape)\n",
        " \n",
        "  x1, x2 = R.nonzero()\n",
        "  \n",
        "  M[x1, x2] = 1\n",
        " \n",
        " \n",
        " # P_tau = P[x1, :]\n",
        " # Q_tau = Q[x2, :]\n",
        " \n",
        "  # Inner Product\n",
        "  #prod = np.sum((P_tau * Q_tau), axis = 1)\n",
        " \n",
        " \n",
        "  #R_hat[x1, x2] = prod\n",
        " \n",
        "  #res = np.multiply(R - R_hat, M)\n",
        " \n",
        "  R_hat = P @ Q.T\n",
        " \n",
        " \n",
        "  res = np.multiply(R_hat - R, M)\n",
        " \n",
        "  gradient = alpha * (lam * P - (res @ Q))\n",
        "  P += gradient\n",
        " \n",
        "  return P\n",
        " \n",
        " \n",
        "def update_Q(P, Q, R, alpha = 0.001, lam = 0.001):\n",
        "  \n",
        "  # assert R.shape == R_hat.shape, \"R shoud equal to R_hat\"\n",
        "  assert P.shape[1] == Q.shape[1], \"P and Q should have proper dimensions for matrix multiplication\"\n",
        " \n",
        "  M = np.zeros(R.shape)\n",
        "  x1, x2 = R.nonzero()\n",
        "  \n",
        "  M[x1, x2] = 1\n",
        " \n",
        " # P_tau = P[x1, :]\n",
        " # Q_tau = Q[x2, :]\n",
        " \n",
        "  # Inner Product\n",
        "  #prod = np.sum((P_tau * Q_tau), axis = 1)\n",
        " \n",
        " \n",
        "  #R_hat[x1, x2] = prod\n",
        " \n",
        "  #res = np.multiply(R_hat - R, M)\n",
        " \n",
        "  R_hat = P @ Q.T\n",
        " \n",
        "  res = np.multiply(R_hat - R, M)\n",
        " \n",
        "  gradient = alpha * (lam * Q - (res.T @ P))\n",
        "  \n",
        "  Q += gradient\n",
        "  \n",
        " \n",
        "  return Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFvMMcOOdwBe",
        "outputId": "0228b2ef-568f-40a1-ccc7-d5d42ec83b06"
      },
      "source": [
        "def calculate_loss(P, Q, R, lam):\n",
        " \n",
        "  R_prod = (P @ Q.T)\n",
        "  \n",
        "  x1, x2 = R.nonzero()\n",
        "  R_hat = R_prod[x1, x2]\n",
        "  R_tau = R.data\n",
        "  \n",
        "  mu = np.mean(R_hat) # Overall average rating\n",
        "  b_u = np.mean(R_prod, axis = 1)\n",
        "  b_i = np.mean(R_prod, axis = 0)\n",
        "  b_u = b_u[x1] - mu\n",
        "  b_i = b_i[x2] - mu\n",
        " \n",
        "  res = R_tau - (R_hat + mu + b_u + b_i)\n",
        " \n",
        "  res = np.square(res)\n",
        " \n",
        "  res = np.mean(res)\n",
        "  res += lam *(np.linalg.norm(P) + np.linalg.norm(Q)) # Regularization Term \n",
        "  return res\n",
        " \n",
        "# Set the dimension of latent space\n",
        "k = 7# 7 \n",
        " \n",
        "# Initialize the matrices P and Q randomly\n",
        "P = np.random.random(size = (n_users, k))\n",
        "Q = np.random.random(size = (n_movies, k))\n",
        " \n",
        "# P = np.zeros((n_users, k))\n",
        "# Q = np.zeros((n_movies, k))\n",
        " \n",
        "# b_u = np.zeros(P.shape[0])\n",
        "# b_i = np.zeros(Q.shape[0])\n",
        " \n",
        " \n",
        "# R_hat = np.zeros(R.shape)\n",
        " \n",
        " \n",
        "# Learning Rate\n",
        "alpha = 0.0006\n",
        "alpha = 1e-6 #0.000001 #40 or 50 or .00045\n",
        " \n",
        "# Regularization rate\n",
        "lam = 0.00001\n",
        " \n",
        "# Variables to keep the best P and Q according to the lowest test loss \n",
        "best_P = P\n",
        "best_Q = Q\n",
        "last_loss = None\n",
        " \n",
        "# Run for epochs\n",
        "for iter in range(50):\n",
        " \n",
        "  # Update Steps for P then Q by following ALS algorithm\n",
        "  P = update_P(P, Q, R, alpha=alpha, lam=lam)\n",
        "  Q = update_Q(P, Q, R, alpha=alpha, lam=lam)\n",
        " \n",
        "  # Regularization rate lam = 0 for calculating the loss of test data R2\n",
        "  # Regularization term is not included in test error\n",
        "  test_loss = calculate_loss(P, Q, R2, lam=0)\n",
        " \n",
        "  # Calculating the loss of training data R\n",
        "  loss = calculate_loss(P, Q, R, lam = lam)\n",
        " \n",
        "  print(f\" epoch {iter}, training error {loss}, test error {test_loss} \") \n",
        " \n",
        "  # A control block for keeping the best P and Q for the lowest test loss during the training\n",
        "  if last_loss:\n",
        "    if last_loss > test_loss:\n",
        "      best_P = P.copy()\n",
        "      best_Q = Q.copy()\n",
        "      last_loss = test_loss\n",
        "    else:\n",
        "      pass\n",
        " \n",
        "  else:\n",
        "    last_loss = test_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " epoch 0, training error 2.208494831012457, test error 2.2602729473608325 \n",
            " epoch 1, training error 2.206067454318581, test error 2.2599594238116456 \n",
            " epoch 2, training error 2.203704138913264, test error 2.259673797827935 \n",
            " epoch 3, training error 2.2014044873311094, test error 2.259415903712117 \n",
            " epoch 4, training error 2.19916810377244, test error 2.2591855764303936 \n",
            " epoch 5, training error 2.1969945940960063, test error 2.258982651609955 \n",
            " epoch 6, training error 2.194883565811776, test error 2.2588069655361784 \n",
            " epoch 7, training error 2.192834628073828, test error 2.258658355149891 \n",
            " epoch 8, training error 2.190847391673324, test error 2.258536658044651 \n",
            " epoch 9, training error 2.188921469031574, test error 2.25844171246407 \n",
            " epoch 10, training error 2.1870564741931804, test error 2.2583733572991647 \n",
            " epoch 11, training error 2.1852520228192707, test error 2.2583314320857433 \n",
            " epoch 12, training error 2.183507732180825, test error 2.258315777001822 \n",
            " epoch 13, training error 2.181823221152059, test error 2.258326232865078 \n",
            " epoch 14, training error 2.1801981102039085, test error 2.258362641130324 \n",
            " epoch 15, training error 2.1786320213975907, test error 2.2584248438870254 \n",
            " epoch 16, training error 2.177124578378234, test error 2.2585126838568357 \n",
            " epoch 17, training error 2.175675406368587, test error 2.258626004391174 \n",
            " epoch 18, training error 2.1742841321628164, test error 2.2587646494688136 \n",
            " epoch 19, training error 2.1729503841203526, test error 2.2589284636935205 \n",
            " epoch 20, training error 2.171673792159834, test error 2.259117292291707 \n",
            " epoch 21, training error 2.170453987753105, test error 2.259330981110113 \n",
            " epoch 22, training error 2.1692906039192943, test error 2.259569376613515 \n",
            " epoch 23, training error 2.1681832752189525, test error 2.2598323258824733 \n",
            " epoch 24, training error 2.1671316377482692, test error 2.2601196766110827 \n",
            " epoch 25, training error 2.1661353291333483, test error 2.260431277104773 \n",
            " epoch 26, training error 2.1651939885245435, test error 2.260766976278116 \n",
            " epoch 27, training error 2.1643072565908805, test error 2.2611266236526704 \n",
            " epoch 28, training error 2.163474775514508, test error 2.261510069354845 \n",
            " epoch 29, training error 2.1626961889852474, test error 2.261917164113784 \n",
            " epoch 30, training error 2.1619711421951693, test error 2.262347759259281 \n",
            " epoch 31, training error 2.1612992818332595, test error 2.2628017067197126 \n",
            " epoch 32, training error 2.1606802560801195, test error 2.2632788590199984 \n",
            " epoch 33, training error 2.160113714602736, test error 2.263779069279578 \n",
            " epoch 34, training error 2.159599308549314, test error 2.264302191210415 \n",
            " epoch 35, training error 2.159136690544149, test error 2.2648480791150165 \n",
            " epoch 36, training error 2.158725514682558, test error 2.2654165878844843 \n",
            " epoch 37, training error 2.158365436525881, test error 2.266007572996573 \n",
            " epoch 38, training error 2.158056113096507, test error 2.2666208905137792 \n",
            " epoch 39, training error 2.1577972028729735, test error 2.267256397081449 \n",
            " epoch 40, training error 2.1575883657851076, test error 2.2679139499258962 \n",
            " epoch 41, training error 2.157429263209217, test error 2.2685934068525566 \n",
            " epoch 42, training error 2.157319557963329, test error 2.26929462624414 \n",
            " epoch 43, training error 2.1572589143024787, test error 2.2700174670588216 \n",
            " epoch 44, training error 2.157246997914043, test error 2.2707617888284366 \n",
            " epoch 45, training error 2.1572834759131188, test error 2.2715274516567 \n",
            " epoch 46, training error 2.157368016837956, test error 2.272314316217439 \n",
            " epoch 47, training error 2.157500290645414, test error 2.2731222437528515 \n",
            " epoch 48, training error 2.1576799687064825, test error 2.2739510960717664 \n",
            " epoch 49, training error 2.157906723801829, test error 2.274800735547936 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNQCJztiAlBZ",
        "outputId": "1645c47a-e807-4780-e67b-df6e032db9ba"
      },
      "source": [
        "# Calculate the test loss using the best values in P and Q\n",
        "calculate_loss(best_P, best_Q, R2, lam=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.274492146337732"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BgBlYE89Bbs"
      },
      "source": [
        " \n",
        "MODELS_PATH = \"models\"\n",
        "MODELS_PATH = \".\"\n",
        "with open(path_join(MODELS_PATH, \"P_ARRAY_CF.npy\"), \"wb\") as f:\n",
        "  np.save(f, P)\n",
        "with open(path_join(MODELS_PATH, \"Q_ARRAY_CF.npy\"), \"wb\") as f:\n",
        "  np.save(f, Q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "amutrkA--qAi",
        "outputId": "ce3ff9f1-5e28-4f1a-e0cb-4dd6d632128b"
      },
      "source": [
        "def do_recommendation(user, P=best_P, Q=best_Q, n=5):\n",
        "  '''Returns a list of top n recommendations (movies or items) given id of the user\n",
        "  Args:\n",
        "    user (number): The user id\n",
        "    P (np.ndarray): The First matrix P in factorization equation R = P @ Q.T\n",
        "    Q (np.ndarray): The Second matrix Q in factorization equation R = P @ Q.T\n",
        "    n (number): Number of retrieved elements \n",
        " \n",
        "  Returns:\n",
        "    list: a list of ids of top n recommendations\n",
        "    list: a list of ratings of top n recommendations\n",
        " \n",
        "  '''\n",
        " \n",
        "  R_hat = P @ Q.T # Calculate predicted ratings\n",
        " \n",
        "  ratings = R_hat[user, :] # Select the ratings of the specific user\n",
        " \n",
        "  ids = np.argsort(ratings)[-n:][::-1] # Returns the top n ratings \n",
        " \n",
        " \n",
        "  return ids, ratings[ids]\n",
        " \n",
        " \n",
        "def recommend(user_id, P, Q, top_n, original_item_ids):\n",
        "  '''Returns a data frame consists of two columns, the first column\n",
        "  involves the ids of top n recommendations (items or movies), given the user id\n",
        " \n",
        "  Args:\n",
        "    user_id (number): The user id\n",
        "    P (np.ndarray): The First matrix P in factorization equation R = P @ Q.T\n",
        "    Q (np.ndarray): The Second matrix Q in factorization equation R = P @ Q.T\n",
        "    top_n (number): Number of retrieved elements \n",
        "    original_item_ids: a list of original ids of the items before mappingthem to basic indexing\n",
        " \n",
        "  Returns:\n",
        "    pd.DataFrame: a data frame consists of two columns, the first column\n",
        "  involves the ids of top n recommendations (items or movies), given the user id\n",
        " \n",
        "  '''\n",
        " \n",
        "  # Do recommendation\n",
        "  ids, ratings= do_recommendation(user_id, P, Q, top_n)\n",
        " \n",
        "  # Remaps the ids of movies to its original index\n",
        "  result = unmap_ids(pd.Series(ids), original_item_ids)\n",
        " \n",
        "  result = pd.DataFrame({'Item':result.values, 'Rating':ratings})\n",
        " \n",
        "  return result\n",
        "  \n",
        " \n",
        "# Usage\n",
        " \n",
        "recommend(0, best_P, best_Q, 10, train_df['movieId'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Item</th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8831</td>\n",
              "      <td>2.950981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>45950</td>\n",
              "      <td>2.949545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>164</td>\n",
              "      <td>2.939739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>106100</td>\n",
              "      <td>2.916010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1665</td>\n",
              "      <td>2.898494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5637</td>\n",
              "      <td>2.897347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4429</td>\n",
              "      <td>2.893091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8914</td>\n",
              "      <td>2.885189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>60037</td>\n",
              "      <td>2.869712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>5949</td>\n",
              "      <td>2.869378</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Item    Rating\n",
              "0    8831  2.950981\n",
              "1   45950  2.949545\n",
              "2     164  2.939739\n",
              "3  106100  2.916010\n",
              "4    1665  2.898494\n",
              "5    5637  2.897347\n",
              "6    4429  2.893091\n",
              "7    8914  2.885189\n",
              "8   60037  2.869712\n",
              "9    5949  2.869378"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVVwIH7oeXFh"
      },
      "source": [
        "# Neural Collabrative Filtering Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPec6uGkyK89"
      },
      "source": [
        "class NCA(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.n_users = config['n_users']\n",
        "    self.n_items = config['n_items']\n",
        "    self.k = config['k']\n",
        " \n",
        "    self.embed_user = nn.Embedding(self.n_users, self.k)\n",
        "    self.embed_item = nn.Embedding(self.n_items, self.k)\n",
        " \n",
        "    self.fc_layers = nn.ModuleList()\n",
        "    for idx, (in_size, out_size) in enumerate(zip(config['layers'][:-1], config['layers'][1:])):\n",
        "        self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
        "        \n",
        "    \n",
        "    self.dropout = nn.Dropout(0.2)\n",
        " \n",
        "    self.output = nn.Linear(config['layers'][-1],  1)\n",
        "    self.output_f = nn.Sigmoid()\n",
        " \n",
        "  def forward(self, users, items):\n",
        "    \n",
        "    # users, items = x\n",
        "    users_x = self.embed_user(users)\n",
        "    items_x = self.embed_item(items)\n",
        " \n",
        "    x = torch.cat([users_x, items_x], dim = 1) # Concatenate along the second axis\n",
        " \n",
        "    for i in range(len(self.fc_layers)):\n",
        "      x = self.fc_layers[i](x)\n",
        "      x = nn.ReLU()(x)\n",
        "      x = self.dropout(x)\n",
        " \n",
        "    x = self.output(x)\n",
        "    x = self.output_f(x) * config['rating_range'] + config['lowest_rating']\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qTriIJ6NCJ_",
        "outputId": "a32c24c6-4ae7-4490-bf88-2956d69de0a2"
      },
      "source": [
        "model(users, movies).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([761972, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfE5J13fk5Tl",
        "outputId": "1ffc684e-b8b1-489f-c0b1-36752d21591f"
      },
      "source": [
        "R.data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(761972,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWR3EEsOp155"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B38_-zCMp0T7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae70efdf-8801-49cc-a6f0-66157c87292f"
      },
      "source": [
        "# Latent Space Dimension\n",
        "k = 7\n",
        " \n",
        "config = {\n",
        "    'n_users':n_users, # Number of Users\n",
        "    'n_items': n_movies, # Number of Items\n",
        "    'k': k, # Latent Space Dimension\n",
        "    'layers':[k * 2, 64, 16, 8],  # sizes of fully connected layers\n",
        "    'rating_range': 4,  # Range of rating (5 - 1 = 4)\n",
        "    'lowest_rating':1 # The lowest rating (1)\n",
        "    }\n",
        " \n",
        "# Input Data\n",
        "users = torch.Tensor(user_ids).int()\n",
        "movies = torch.Tensor(movie_ids).int()\n",
        "ratings = torch.Tensor(R.data)\n",
        " \n",
        " \n",
        "# Try to use GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        " \n",
        " \n",
        "# Model\n",
        "model = NCA(config).to(device)\n",
        " \n",
        "# Do one-hot encoding\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NCA(\n",
              "  (embed_user): Embedding(6687, 7)\n",
              "  (embed_item): Embedding(5064, 7)\n",
              "  (fc_layers): ModuleList(\n",
              "    (0): Linear(in_features=14, out_features=64, bias=True)\n",
              "    (1): Linear(in_features=64, out_features=16, bias=True)\n",
              "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (output): Linear(in_features=8, out_features=1, bias=True)\n",
              "  (output_f): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s99dKLf5MfIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cd6f526-6a55-446b-e0c7-dcfee4148092"
      },
      "source": [
        " \n",
        "learning_rate = 0.001\n",
        "critertion = nn.MSELoss()\n",
        "batch_size = 100\n",
        "epochs = range(40)\n",
        " \n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        " \n",
        "data_loader = DataLoader(TensorDataset(users, movies, ratings), batch_size = batch_size)  \n",
        "losses = []\n",
        "for epoch in epochs:\n",
        "  epoch_loss = []\n",
        "  for batch_users, batch_movies, batch_ratings in data_loader:\n",
        " \n",
        "    users = batch_users.to(device)\n",
        "    movies = batch_movies.to(device)\n",
        "    ratings = batch_ratings.to(device)\n",
        " \n",
        "    optimizer.zero_grad()\n",
        " \n",
        "    output = model(users, movies)[:, 0]\n",
        " \n",
        "    loss = critertion(output, ratings)\n",
        " \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    epoch_loss.append(loss.item())\n",
        " \n",
        "  avg_epoch_loss = np.mean(epoch_loss)\n",
        "  losses.append(avg_epoch_loss)\n",
        "  print(f\"epoch {epoch}, loss = {avg_epoch_loss}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, loss = 1.0592916649001165\n",
            "epoch 1, loss = 0.9406692212491523\n",
            "epoch 2, loss = 0.9122387819745018\n",
            "epoch 3, loss = 0.897270367641657\n",
            "epoch 4, loss = 0.8870583937168512\n",
            "epoch 5, loss = 0.8783762987204424\n",
            "epoch 6, loss = 0.8677011451050834\n",
            "epoch 7, loss = 0.8565393227418968\n",
            "epoch 8, loss = 0.8476709221291729\n",
            "epoch 9, loss = 0.8362611738705807\n",
            "epoch 10, loss = 0.8246295854869712\n",
            "epoch 11, loss = 0.8158248318647697\n",
            "epoch 12, loss = 0.8061579541173663\n",
            "epoch 13, loss = 0.7987824181392984\n",
            "epoch 14, loss = 0.790384490298122\n",
            "epoch 15, loss = 0.7837509512265758\n",
            "epoch 16, loss = 0.7776246184087175\n",
            "epoch 17, loss = 0.7718029411700298\n",
            "epoch 18, loss = 0.7663959458147681\n",
            "epoch 19, loss = 0.7614975130200151\n",
            "epoch 20, loss = 0.7574817955924145\n",
            "epoch 21, loss = 0.7539870116077462\n",
            "epoch 22, loss = 0.7498597814811496\n",
            "epoch 23, loss = 0.7468343569889782\n",
            "epoch 24, loss = 0.7444679163846131\n",
            "epoch 25, loss = 0.7414363801137122\n",
            "epoch 26, loss = 0.7385940946892804\n",
            "epoch 27, loss = 0.7366433689414751\n",
            "epoch 28, loss = 0.7345741606388348\n",
            "epoch 29, loss = 0.7314737587590309\n",
            "epoch 30, loss = 0.7302808903138156\n",
            "epoch 31, loss = 0.7284422229486579\n",
            "epoch 32, loss = 0.726393522786617\n",
            "epoch 33, loss = 0.7249089819005156\n",
            "epoch 34, loss = 0.7233761316287549\n",
            "epoch 35, loss = 0.7224121814432342\n",
            "epoch 36, loss = 0.7206684369388527\n",
            "epoch 37, loss = 0.7181076794549862\n",
            "epoch 38, loss = 0.7171780290693202\n",
            "epoch 39, loss = 0.7165827309424326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ_vOOWRmkvM",
        "outputId": "cd4247f1-d263-4a86-b8b0-a814db9d7bf7"
      },
      "source": [
        "critertion(model(users, movies)[:,0], ratings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.3075, grad_fn=<MseLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhgTKgsA_BPI"
      },
      "source": [
        " \n",
        "path = path_join(MODELS_PATH, \"acf.pth\")\n",
        " \n",
        "torch.save(model.state_dict, path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}